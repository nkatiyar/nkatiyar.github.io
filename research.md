---
layout: default
title: Current Projects
desc: Check out what we're working on
nav: Research
---

# How do neurons see the world?

Typical neuroscience experiments start by assuming we know the set of variables that drive neural activity. But what if neurons are tuned to variables we would never have guessed? What if, as with social interaction, the stimulus set is too complex to be boiled down to a few dimensions. With [Jeff Beck](https://www.neuro.duke.edu/research/faculty-labs/beck-lab), we're [developing models](http://arxiv.org/abs/1512.01408) that infer stimulus categories *directly from data*, allowing us to "tag" images and movies based on neural responses.
<div class="row">
  <div class="col-md-4">
    <figure>
        <img src="https://web.duke.edu/mind/level2/faculty/pearson/assets/images/website/cbp_model.svg" class="img-responsive">
        <figcaption>
            Neural responses are sums of sensitivities to binary image "tags."
        </figcaption>
    </figure>
  </div>
  <div class="col-md-8">
    <figure>
        <img src="https://web.duke.edu/mind/level2/faculty/pearson/assets/images/website/imgclust_web.svg" class="img-responsive">
        <figcaption>
            In an example dataset, the model correctly tagged monkey faces, whole monkeys, and monkey body parts.
        </figcaption>
    </figure>
  </div>
</div>

# Opening up the skull

For most neuroscientists, studying the brain means either using noninvasive brain imaging techniques like EEG or fMRI in humans or using more precise but invasive methods in animal models. However, when patients with Parkinson's Disease or epilepsy undergo brain surgery, we have a rare opportunity to study brain activity at the most detailed level in human beings. Together with collaborators in [neurosurgery, neurology, and biomedical engineering](people.html#collaborators) we're investigating the processes underlying self-control, social interaction, and language with a precision rarely available in humans.


# Eye tracking unplugged

Where we look speaks volumes about what we're thinking. For over a century, psychologists and neurobiologists have used the movements of the eyes and measurements of pupil size to study the mind, but the need for experimental control has limited our ability to study eye movements in naturalistic settings. In P[&lambda;]ab, we are pairing new advances in [eye tracking](http://www.tobiipro.com/product-listing/tobii-pro-glasses-2/) technology with methods in [computer vision](blog/2015/11/06/eye_tracking_tech.html) and machine learning to tackle the challenge of studying eye movements in real-world settings, with applications ranging from treatment of acute fear to how we view art.

<div class="row">
  <div class="col-md-4">
    <figure>
        <div class="video-container">
            <iframe width="730" height="410" src="https://www.youtube.com/embed/E6c9Z0Mkc-E?rel=0&amp;showinfo=0" frameborder="0" allowfullscreen></iframe>
        </div>
        <figcaption>
            Mapping gaze between three and two dimensions.
        </figcaption>
    </figure>
  </div>
  <div class="col-md-4">
    <figure>
        <div class="video-container">
            <iframe width="730" height="410" src="https://www.youtube.com/embed/fSl6FiyHTes?rel=0&amp;showinfo=0" frameborder="0" allowfullscreen></iframe>
        </div>
        <figcaption>
            Gaze mapping free viewing of art.
        </figcaption>
    </figure>
  </div>
  <div class="col-md-4" style="padding: 0 0 0 0">
    <figure>
        <a href="http://jeffmacinnes.com/research/gazeMapping/sonhouse3D/index.html">
            <img src="https://web.duke.edu/mind/level2/faculty/pearson/assets/images/website/dynamicGaze.png" class="img-responsive" style="margin: 7 0 7 0">
        </a>
        <figcaption>
            Three dimensional reconstruction of viewer position.
        </figcaption>
    </figure>
  </div>
</div>
